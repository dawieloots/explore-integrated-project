{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dawieloots/explore-integrated-project/blob/main/advanced_classification_predict_COLAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0034c475-0962-4dcc-934f-049f92c09cb3",
      "metadata": {
        "id": "0034c475-0962-4dcc-934f-049f92c09cb3"
      },
      "source": [
        "# ADVANCED CLASSIFICATION PREDICT\n",
        "#### By Dawie Loots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12042635-3289-40ca-82f7-56c93dac89b6",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "12042635-3289-40ca-82f7-56c93dac89b6"
      },
      "source": [
        "### Honour Code\n",
        "\n",
        "I, Dawie Loots, confirm - by submitting this document - that the solutions in this notebook are a result of my own work and that I abide by the [EDSA honour code](https://drive.google.com/file/d/1QDCjGZJ8-FmJE3bZdIQNwnJyQKPhHZBn/view?usp=sharing).\n",
        "\n",
        "Non-compliance with the honour code constitutes a material breach of contract."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20841b8a-2fde-431a-9590-7047a4b486fc",
      "metadata": {
        "id": "20841b8a-2fde-431a-9590-7047a4b486fc"
      },
      "source": [
        "<a id=\"cont\"></a>\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "<a href=#one>1. Predict overview</a>\n",
        "\n",
        "<a href=#two>2. Importing packages</a>\n",
        "\n",
        "<a href=#three>3. Loading the data</a>\n",
        "\n",
        "<a href=#four>4. Data Preprocessing</a>\n",
        "\n",
        "<a href=#five>5. Exploratory Data Analysis</a>\n",
        "\n",
        "<a href=#six>6. Modeling</a>\n",
        "\n",
        "<a href=#seven>7. Model performance evaluation</a>\n",
        "\n",
        "<a href=#eight>8. Model analysis and conclusion</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e51577-c2e5-46cd-84bf-c442505f77a5",
      "metadata": {
        "id": "f3e51577-c2e5-46cd-84bf-c442505f77a5"
      },
      "source": [
        "<a id=\"one\"></a>\n",
        "### 1. Predict overview\n",
        "\n",
        "Many companies are built around lessening oneâ€™s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
        "\n",
        "With this context, EA is challenging you during the Classification Sprint with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
        "\n",
        "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fabfd0c-c251-4974-8b2a-3d768ce46ed5",
      "metadata": {
        "id": "6fabfd0c-c251-4974-8b2a-3d768ce46ed5"
      },
      "source": [
        "<a id=\"two\"></a>\n",
        "### 2. Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bad7e9a6-467d-45f7-9231-da6134ab143d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bad7e9a6-467d-45f7-9231-da6134ab143d",
        "outputId": "768a593f-b2da-4baf-e457-543ad6080196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Libraries for data loading, data manipulation and data visulisation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import chardet # To provide a best estimate of the encoding that was used in the text data\n",
        "import io # For string operations\n",
        "%matplotlib inline\n",
        "\n",
        "# Libraries for data preparation and model building\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import math\n",
        "import re\n",
        "from sklearn.utils import resample\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Setting global constants to ensure notebook results are reproducible\n",
        "PARAMETER_CONSTANT = 42  # This is the seed value for random number generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3762b8e-2add-425f-8678-9dcb6f373fcc",
      "metadata": {
        "id": "b3762b8e-2add-425f-8678-9dcb6f373fcc"
      },
      "source": [
        "<a id=\"three\"></a>\n",
        "### 3. Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76838241-36bb-4953-bbb9-0d0dbd5e44e0",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "76838241-36bb-4953-bbb9-0d0dbd5e44e0",
        "outputId": "68fc4413-a5f1-4167-b79e-f22a66d1f4f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-09a3d6262287>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "df_train = pd.read_csv('train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c80fe6-a9f0-4c8b-a5bd-63f35f2aa847",
      "metadata": {
        "id": "d5c80fe6-a9f0-4c8b-a5bd-63f35f2aa847"
      },
      "outputs": [],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d6131b-fd90-4cce-8b01-acb12fb175af",
      "metadata": {
        "id": "f5d6131b-fd90-4cce-8b01-acb12fb175af"
      },
      "source": [
        "<a id=\"four\"></a>\n",
        "### 4. Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e004d5b-c9f3-4581-a49d-027009259959",
      "metadata": {
        "id": "0e004d5b-c9f3-4581-a49d-027009259959"
      },
      "source": [
        "Check for missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a70b03ae-824c-42f0-907a-d9e8327dc16b",
      "metadata": {
        "scrolled": true,
        "id": "a70b03ae-824c-42f0-907a-d9e8327dc16b"
      },
      "outputs": [],
      "source": [
        "df_train.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2126f41-6fc6-4aee-a1ee-12e6e7b54d86",
      "metadata": {
        "id": "b2126f41-6fc6-4aee-a1ee-12e6e7b54d86"
      },
      "source": [
        "There is no missing data, so let's proceed by checking for class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5efdf0e3-5f0f-4394-9828-521f736c0e0a",
      "metadata": {
        "id": "5efdf0e3-5f0f-4394-9828-521f736c0e0a"
      },
      "outputs": [],
      "source": [
        "class_count = df_train['sentiment'].value_counts()\n",
        "class_count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86ac1b6f-ec7c-470a-9d79-ff95cdcf3df1",
      "metadata": {
        "id": "86ac1b6f-ec7c-470a-9d79-ff95cdcf3df1"
      },
      "source": [
        "Seems like most of the tweets were for class 1 (supporting the belief of man-made changes)\n",
        "Let's divide the total 15,819 tweets by 4, to get +- 3,955 per class.  We will need to upsamle classes 0, -1 and 2, and downsample class 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f16beeed-2167-42ce-92c2-be317a239918",
      "metadata": {
        "id": "f16beeed-2167-42ce-92c2-be317a239918"
      },
      "outputs": [],
      "source": [
        "class_min1 = df_train[df_train['sentiment']==-1]\n",
        "class_0 = df_train[df_train['sentiment']==0]\n",
        "class_1 = df_train[df_train['sentiment']==1]\n",
        "class_2 = df_train[df_train['sentiment']==2]\n",
        "balance = len(df_train) // 4 # The number of samples that will result in class balance\n",
        "df_train_class1_resampled = resample(class_1,\n",
        "                            replace=False, # sample without replacement (no need to duplicate observations)\n",
        "                            n_samples=balance, # make all classes equal\n",
        "                            random_state=27) # reproducible results\n",
        "df_train_classmin1_resampled = resample(class_min1,\n",
        "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
        "                            n_samples=balance, # make all classes equal\n",
        "                            random_state=27) # reproducible results\n",
        "df_train_class0_resampled = resample(class_0,\n",
        "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
        "                            n_samples=balance, # make all classes equal\n",
        "                            random_state=27) # reproducible results\n",
        "df_train_class2_resampled = resample(class_2,\n",
        "                            replace=True, # sample with replacement (we need to duplicate observations)\n",
        "                            n_samples=balance, # make all classes equal\n",
        "                            random_state=27) # reproducible results\n",
        "\n",
        "df_train.reset_index(drop=True, inplace=True) # Reset index before upsampling\n",
        "df_train = pd.concat([df_train_class1_resampled, df_train_classmin1_resampled,\n",
        "                                df_train_class0_resampled, df_train_class2_resampled])\n",
        "df_train.set_index(df_train.index, inplace=True) # Set the default integer index as the new index after upsampling\n",
        "\n",
        "# Check new class counts\n",
        "df_train['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749b6a1c-d625-4153-87b0-cb821c6d09c7",
      "metadata": {
        "id": "749b6a1c-d625-4153-87b0-cb821c6d09c7"
      },
      "source": [
        "Now that we have class balance, let's proceed with the following steps to convert text into numerical values, so that it can be used for this classification task:\n",
        "\n",
        "- Removing noise (such as web-urls)\n",
        "- Removing punctuation\n",
        "- Tokenization\n",
        "- Removal of stop words\n",
        "- Lemmatization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad8fee8-859c-4382-9c96-21a61c44d114",
      "metadata": {
        "id": "6ad8fee8-859c-4382-9c96-21a61c44d114"
      },
      "outputs": [],
      "source": [
        "# Remove noise (all hyperlinks)\n",
        "\n",
        "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'   # Find all hyperlinks\n",
        "subs_url = r''\n",
        "df_train['message'] = df_train['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6cfba5-dfe3-4d55-8051-70dab081df9c",
      "metadata": {
        "scrolled": true,
        "id": "bd6cfba5-dfe3-4d55-8051-70dab081df9c"
      },
      "outputs": [],
      "source": [
        "# Handle emoticons\n",
        "\n",
        "emoticon_dictionary = {':\\)': 'smiley_face_emoticon',\n",
        "                       ':\\(': 'frowning_face_emoticon',\n",
        "                       ':D': 'grinning_face_emoticon',\n",
        "                       ':P': 'sticking_out_tongue_emoticon',\n",
        "                       ';\\)': 'winking_face_emoticon',\n",
        "                       ':o': 'surprised_face_emoticon',\n",
        "                       ':\\|': 'neutral_face_emoticon',\n",
        "                       ':\\'\\)': 'tears_of_joy_emoticon',\n",
        "                       ':\\'\\(': 'crying_face_emoticon'}\n",
        "\n",
        "df_train['message_encoded_emojis'] = df_train['message'].replace(emoticon_dictionary, regex=True)\n",
        "# Check if it was correctly\n",
        "emoji_rows = df_train[df_train['message'].str.contains(':\\(')]\n",
        "emoji_rows.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9402beb4-2117-4d0d-b77c-4b6e2ab4cb78",
      "metadata": {
        "id": "9402beb4-2117-4d0d-b77c-4b6e2ab4cb78"
      },
      "outputs": [],
      "source": [
        "# Remove punctuation and expand all contracted words\n",
        "def remove_punctuation(message):\n",
        "    contractions = {\"'t\": \" not\",\"'s\": \" is\",\"'re\": \" are\",\"'ll\": \" will\", \"'m\": \" am\"}\n",
        "    pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(key) for key in contractions.keys()) + r\")\\b\")\n",
        "    message = re.sub(r\"n't\\b\", \" not\", message) # Replace \"n't\" with \" not\"\n",
        "    message = pattern.sub(lambda match: contractions[match.group(0)], message) # Replace all other contractions except for \"n't\"\n",
        "    return ''.join([l for l in message if l not in string.punctuation])\n",
        "\n",
        "df_train['message_clean'] = df_train['message_encoded_emojis'].apply(remove_punctuation)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b14d0b-108c-4e62-b980-d6978c6b2e13",
      "metadata": {
        "scrolled": true,
        "id": "23b14d0b-108c-4e62-b980-d6978c6b2e13"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "tokenizer = TweetTokenizer()\n",
        "df_train['tokens'] = df_train['message_clean'].apply(tokenizer.tokenize)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dbe356-b0a5-4a8b-b88e-98414bafcada",
      "metadata": {
        "id": "b8dbe356-b0a5-4a8b-b88e-98414bafcada"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "def remove_stop_words(tokens):\n",
        "    return [t for t in tokens if t not in stopwords.words('english')]\n",
        "\n",
        "df_train['tokens_without_stopwords'] = df_train['tokens'].apply(remove_stop_words)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18b54f55-5c16-4987-b356-db7e6fdd6764",
      "metadata": {
        "scrolled": true,
        "id": "18b54f55-5c16-4987-b356-db7e6fdd6764"
      },
      "outputs": [],
      "source": [
        "# Lemmatization\n",
        "\n",
        "def mbti_lemma(words, lemmatizer):\n",
        "    return [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df_train['lemma'] = df_train['tokens_without_stopwords'].apply(mbti_lemma, args=(lemmatizer, ))\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca7b0375-22d2-417c-9696-1c1779ef096c",
      "metadata": {
        "id": "ca7b0375-22d2-417c-9696-1c1779ef096c"
      },
      "source": [
        "<a id=\"five\"></a>\n",
        "### 5. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f14cbb6-bc2e-4b8c-a950-99c82278c463",
      "metadata": {
        "id": "8f14cbb6-bc2e-4b8c-a950-99c82278c463"
      },
      "outputs": [],
      "source": [
        "# Convert into Bag Of Words\n",
        "\n",
        "# Flatten the list of lists into a single list of strings\n",
        "df_train['flattened_lemma'] = df_train['lemma'].apply(lambda word_list: ' '.join(word_list))\n",
        "df_train.head()\n",
        "\n",
        "# Create and fit the CountVectorizer\n",
        "vect1 = CountVectorizer(lowercase=True,max_df=0.5, min_df=2,ngram_range=(1,1), max_features=200)\n",
        "vect2 = CountVectorizer(lowercase=True,max_df=0.5, min_df=2,ngram_range=(1,2), max_features=200)\n",
        "vect3 = CountVectorizer(lowercase=True,max_df=0.5, min_df=2,ngram_range=(1,3), max_features=200)\n",
        "vect1.fit(df_train['flattened_lemma'])\n",
        "vect2.fit(df_train['flattened_lemma'])\n",
        "vect3.fit(df_train['flattened_lemma'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b7c7fca-ef69-4af4-870a-f490c42d06ed",
      "metadata": {
        "scrolled": true,
        "id": "7b7c7fca-ef69-4af4-870a-f490c42d06ed"
      },
      "outputs": [],
      "source": [
        "X1 = vect1.transform(df_train['flattened_lemma'])\n",
        "bag_of_words1 = pd.DataFrame(X1.toarray(), columns=vect1.get_feature_names_out())\n",
        "X2 = vect2.transform(df_train['flattened_lemma'])\n",
        "bag_of_words2 = pd.DataFrame(X2.toarray(), columns=vect2.get_feature_names_out())\n",
        "X3 = vect3.transform(df_train['flattened_lemma'])\n",
        "bag_of_words3 = pd.DataFrame(X3.toarray(), columns=vect3.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c39e72-c7d9-4dfd-aec0-d77c64f5e6b3",
      "metadata": {
        "id": "86c39e72-c7d9-4dfd-aec0-d77c64f5e6b3"
      },
      "outputs": [],
      "source": [
        "df_train.index.is_unique\n",
        "#bag_of_words2.index.is_unique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "553f0698-be51-4992-939d-52ee8d297903",
      "metadata": {
        "scrolled": true,
        "id": "553f0698-be51-4992-939d-52ee8d297903"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "#word_count2 = pd.DataFrame()\n",
        "abridged_train_df = df_train[['message', 'sentiment']]\n",
        "bag_of_words2.reset_index(drop=True, inplace=True)\n",
        "abridged_train_df.reset_index(drop=True, inplace=True)\n",
        "word_count2 = pd.concat([bag_of_words2, abridged_train_df],axis=1)\n",
        "grouped2 = word_count2.groupby('sentiment').sum()\n",
        "top_n = 10\n",
        "top_words_per_class2 = {}\n",
        "for class_name, row in grouped2.iterrows():\n",
        "    top_words2 = row.sort_values(ascending=False)[:top_n]\n",
        "    top_words_per_class2[class_name] = top_words2\n",
        "\n",
        "# Create bar plots for top 20 words per class\n",
        "for class_name, top_words in top_words_per_class2.items():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    top_words.plot(kind='bar', color='skyblue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title(f'Top {top_n} Words in {class_name}')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69f347f-2108-4c7e-a05f-56d7aeb03310",
      "metadata": {
        "id": "a69f347f-2108-4c7e-a05f-56d7aeb03310"
      },
      "source": [
        "<a id=\"six\"></a>\n",
        "### 6. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb29688-0b6a-40c7-a11f-174eb1de1209",
      "metadata": {
        "id": "1eb29688-0b6a-40c7-a11f-174eb1de1209"
      },
      "outputs": [],
      "source": [
        "# Split into training and test data\n",
        "X = word_count2.copy()\n",
        "X.drop(columns=['sentiment','message'],inplace=True)\n",
        "y = word_count2.sentiment\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b615264-745c-4d5e-aeb1-5566eb6df8df",
      "metadata": {
        "id": "3b615264-745c-4d5e-aeb1-5566eb6df8df"
      },
      "outputs": [],
      "source": [
        "names = ['Logistic Regression', 'Nearest Neighbors',\n",
        "         'Linear SVM', 'RBF SVM',\n",
        "         'Decision Tree', 'Random Forest',  'AdaBoost']\n",
        "\n",
        "classifiers = [LogisticRegression(max_iter=1000),\n",
        "               KNeighborsClassifier(3),\n",
        "               SVC(kernel=\"linear\", C=0.025),\n",
        "               SVC(gamma=2, C=1),\n",
        "               DecisionTreeClassifier(max_depth=5),\n",
        "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "               AdaBoostClassifier()\n",
        "               ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c56e167-6788-4219-a6e4-e75a5054cdbd",
      "metadata": {
        "id": "7c56e167-6788-4219-a6e4-e75a5054cdbd"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "models = {}\n",
        "confusion = {}\n",
        "class_report = {}\n",
        "\n",
        "\n",
        "for name, clf in zip(names, classifiers):\n",
        "    print ('Fitting {:s} model...'.format(name))\n",
        "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
        "\n",
        "    print ('... predicting')\n",
        "    y_pred = clf.predict(X_train)\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    print ('... scoring')\n",
        "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
        "    precision = metrics.precision_score(y_train, y_pred, average='weighted')\n",
        "    recall    = metrics.recall_score(y_train, y_pred, average='weighted')\n",
        "\n",
        "    f1        = metrics.f1_score(y_train, y_pred, average='weighted')\n",
        "    f1_test   = metrics.f1_score(y_test, y_pred_test, average='weighted')\n",
        "\n",
        "    # Save the results to dictionaries\n",
        "    models[name] = clf\n",
        "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
        "    class_report[name] = metrics.classification_report(y_train, y_pred)\n",
        "\n",
        "    results.append([name, accuracy, precision, recall, f1, f1_test, run_time.best])\n",
        "\n",
        "\n",
        "results = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Train', 'F1 Test', 'Train Time'])\n",
        "results.set_index('Classifier', inplace= True)\n",
        "\n",
        "print ('... All done!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15ad4ef0-96f4-4df5-962b-37e75a29ebe3",
      "metadata": {
        "id": "15ad4ef0-96f4-4df5-962b-37e75a29ebe3"
      },
      "outputs": [],
      "source": [
        "results.sort_values('F1 Train', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086c6f3f-24cf-47e4-a394-5277e5bbfc8e",
      "metadata": {
        "id": "086c6f3f-24cf-47e4-a394-5277e5bbfc8e"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "results.sort_values('F1 Train', ascending=False, inplace=True)\n",
        "results.plot(y=['F1 Test'], kind='bar', ax=ax[0], xlim=[0,1.1], ylim=[0.85,0.92])\n",
        "results.plot(y='Train Time', kind='bar', ax=ax[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29b66a1c-fbd6-42cd-b6aa-71dc8abe7bfd",
      "metadata": {
        "id": "29b66a1c-fbd6-42cd-b6aa-71dc8abe7bfd"
      },
      "source": [
        "<a id=\"seven\"></a>\n",
        "### 7. Model performance evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfb8b10-fa19-4c84-9c34-0c6d485b6291",
      "metadata": {
        "id": "7cfb8b10-fa19-4c84-9c34-0c6d485b6291"
      },
      "outputs": [],
      "source": [
        "# Use K-Fold cross validation\n",
        "\n",
        "model = models['Logistic Regression']\n",
        "print(cross_val_score(model, X.values, y.values))\n",
        "\n",
        "cv = []\n",
        "for name, model in models.items():\n",
        "    print ()\n",
        "    print(name)\n",
        "    scores = cross_val_score(model, X=X.values, y=y.values, cv=10)\n",
        "    print(\"Accuracy: {:0.2f} (+/- {:0.4f})\".format(scores.mean(), scores.std()))\n",
        "    cv.append([name, scores.mean(), scores.std() ])\n",
        "\n",
        "cv = pd.DataFrame(cv, columns=['Model', 'CV_Mean', 'CV_Std_Dev'])\n",
        "cv.set_index('Model', inplace=True)\n",
        "\n",
        "cv.plot(y='CV_Mean', yerr='CV_Std_Dev',kind='bar', ylim=[0.25, 0.85])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv.plot(y='CV_Mean', yerr='CV_Std_Dev',kind='bar', ylim=[0.25, 0.85])"
      ],
      "metadata": {
        "id": "fXHs2e0oXH5Z"
      },
      "id": "fXHs2e0oXH5Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBAJyRBmXQbg"
      },
      "id": "xBAJyRBmXQbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a8060330-bd6a-4e1c-b2d5-1c18930dcfae",
      "metadata": {
        "id": "a8060330-bd6a-4e1c-b2d5-1c18930dcfae"
      },
      "source": [
        "<a id=\"eight\"></a>\n",
        "### 8. Model analysis and conclusion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GridSearchCV for SVM (RBF)\n",
        "param_grid = {'kernel': ['rbf'],\n",
        "              'gamma': (0.5, 1,2),\n",
        "              'C': (0.5,0.75, 1.0)}\n",
        "svm = SVC()\n",
        "clf = GridSearchCV(svm, param_grid, scoring='f1_macro', cv=2)\n",
        "clf.fit(X_train, y_train)\n",
        "clf.best_params_"
      ],
      "metadata": {
        "id": "aR-QW6oQn7Ik"
      },
      "id": "aR-QW6oQn7Ik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain on best params\n",
        "\n",
        "svm = SVC(kernel='rbf', gamma=1.0, C=1.0)\n",
        "clf = svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "\n",
        "print ('... scoring')\n",
        "accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
        "precision = metrics.precision_score(y_train, y_pred, average='weighted')\n",
        "recall    = metrics.recall_score(y_train, y_pred, average='weighted')\n",
        "\n",
        "f1        = metrics.f1_score(y_train, y_pred, average='weighted')\n",
        "f1_test   = metrics.f1_score(y_test, y_pred_test, average='weighted')\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'f1: {f1}')\n",
        "print(f'f1-test: {f1_test}')\n"
      ],
      "metadata": {
        "id": "QW5dXNEFsxWG"
      },
      "id": "QW5dXNEFsxWG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}